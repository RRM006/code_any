# -*- coding: utf-8 -*-
"""Problem#2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oq7dgZco7_LBawB5ou9YbZsN9R77NmPN
"""

# Step 1.1: Import all required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("All libraries imported successfully!")

# Step 2.1: Load the Dhaka house dataset
# Using a popular Dhaka house price dataset from Kaggle
url = "https://raw.githubusercontent.com/rezacsedu/Dhaka-Housing-Dataset/main/dhaka_house_price.csv"

# Alternative dataset if the above doesn't work:
# url = "https://raw.githubusercontent.com/your-username/dhaka-housing/main/dhaka_houses.csv"

try:
    df = pd.read_csv(url)
    print("‚úÖ Dhaka House Dataset loaded successfully!")
except:
    # If URL fails, we'll create a sample dataset
    print("‚ö†Ô∏è URL not accessible, creating sample Dhaka housing data...")
    np.random.seed(42)
    data = {
        'area': np.random.randint(800, 3000, 500),
        'bedrooms': np.random.randint(1, 6, 500),
        'bathrooms': np.random.randint(1, 4, 500),
        'location_encoded': np.random.randint(1, 10, 500),
        'age': np.random.randint(0, 30, 500),
        'price_lakhs': np.random.uniform(20, 500, 500)
    }
    df = pd.DataFrame(data)
    print("‚úÖ Sample Dhaka housing data created!")

# Step 2.2: Check dataset basic information
print("\nüìä Dataset Overview:")
print("Shape:", df.shape)  # Shows (number_of_rows, number_of_columns)
print("\nFirst 5 rows:")
display(df.head())  # Display first 5 rows

print("\nColumn names:")
print(df.columns.tolist())  # Show all column names

# Step 3.1: Dataset Information
print("üîç DATASET INFORMATION:")
print(df.info())  # Shows data types and non-null counts

print("\n‚ùì MISSING VALUES CHECK:")
print(df.isnull().sum())  # Count missing values in each column

# Step 3.2: Handle missing values if any
if df.isnull().sum().any():
    print("\nüîÑ Handling missing values...")
    # Fill numerical columns with median
    numerical_cols = df.select_dtypes(include=[np.number]).columns
    df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())
    print("Missing values after handling:", df.isnull().sum().sum())

# Step 3.3: Statistical Summary
print("\nüìà STATISTICAL SUMMARY:")
display(df.describe())  # Show mean, std, min, max, quartiles

# Step 3.4: Check the target variable distribution
plt.figure(figsize=(15, 10))

# Plot 1: Target variable distribution
plt.subplot(2, 3, 1)
plt.hist(df.iloc[:, -1], bins=30, edgecolor='black', alpha=0.7, color='skyblue')
plt.title('Distribution of House Prices')
plt.xlabel('Price (Lakhs)')
plt.ylabel('Frequency')

# Plot 2: Correlation heatmap
plt.subplot(2, 3, 2)
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f',
            square=True, cbar_kws={"shrink": .8})
plt.title('Feature Correlation Matrix')
plt.tight_layout()

# Plot 3: Area vs Price scatter plot
plt.subplot(2, 3, 3)
plt.scatter(df['area'], df.iloc[:, -1], alpha=0.6, color='green')
plt.title('Area vs Price')
plt.xlabel('Area (sq ft)')
plt.ylabel('Price (Lakhs)')

# Plot 4: Bedrooms vs Price box plot
plt.subplot(2, 3, 4)
sns.boxplot(x=df['bedrooms'], y=df.iloc[:, -1])
plt.title('Bedrooms vs Price')
plt.xlabel('Number of Bedrooms')
plt.ylabel('Price (Lakhs)')

# Plot 5: Location impact on price
plt.subplot(2, 3, 5)
if 'location_encoded' in df.columns:
    sns.boxplot(x=df['location_encoded'], y=df.iloc[:, -1])
    plt.title('Location vs Price')
    plt.xlabel('Location Code')
    plt.ylabel('Price (Lakhs)')

plt.tight_layout()
plt.show()

# Step 3.5: Pairplot for key features
print("\nüîÑ Generating pairplot...")
# Select key features for pairplot (adjust based on your dataset)
key_features = []
for col in ['area', 'bedrooms', 'bathrooms', 'age', 'location_encoded']:
    if col in df.columns:
        key_features.append(col)
key_features.append(df.columns[-1])  # Add target variable

sns.pairplot(df[key_features], diag_kind='hist', corner=True)
plt.suptitle("Pairwise Relationships - Dhaka Housing", y=1.02)
plt.show()

# Step 4.1: Identify features and target
# Assuming the last column is the price/target variable
X = df.iloc[:, :-1]  # All columns except the last one
y = df.iloc[:, -1]   # The last column as target

print("üéØ FEATURE AND TARGET SETUP:")
print("Features shape:", X.shape)
print("Target shape:", y.shape)
print("\nFeature names:")
print(X.columns.tolist())
print("\nTarget variable:", y.name)

# Step 4.2: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% for testing, 80% for training
    random_state=42     # For reproducible results
)

print("\nüìä DATA SPLITTING COMPLETE:")
print("Training set size:", X_train.shape)
print("Testing set size:", X_test.shape)
print("Training target size:", y_train.shape)
print("Testing target size:", y_test.shape)

# Step 5.1: Initialize Linear Regression model
print("üöÄ TRAINING LINEAR REGRESSION MODEL...")
lin_reg = LinearRegression()

# Step 5.2: Train the model on training data
lin_reg.fit(X_train, y_train)

# Step 5.3: Make predictions on test data
y_pred_lin = lin_reg.predict(X_test)

print("‚úÖ Linear Regression training completed!")
print("Model coefficients:", lin_reg.coef_)
print("Model intercept:", lin_reg.intercept_)

# Step 6.1: Initialize Decision Tree Regressor
print("\nüå≥ TRAINING DECISION TREE REGRESSION MODEL...")
tree_reg = DecisionTreeRegressor(
    max_depth=5,        # Limit depth to prevent overfitting
    random_state=42     # For reproducible results
)

# Step 6.2: Train the Decision Tree model
tree_reg.fit(X_train, y_train)

# Step 6.3: Make predictions on test data
y_pred_tree = tree_reg.predict(X_test)

print("‚úÖ Decision Tree training completed!")

# Step 7.1: Create evaluation function
def evaluate_model(y_true, y_pred, model_name):
    """
    Comprehensive model evaluation function
    """
    print(f"\nüìä === {model_name} Performance ===")
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    print(f"MAE  (Mean Absolute Error) : {mae:.3f}")
    print(f"MSE  (Mean Squared Error)  : {mse:.3f}")
    print(f"RMSE (Root Mean Squared Error): {rmse:.3f}")
    print(f"R¬≤ Score (Goodness of Fit) : {r2:.3f}")

    # Additional metrics
    mean_actual = np.mean(y_true)
    percentage_error = (rmse / mean_actual) * 100
    print(f"RMSE as % of Mean Price   : {percentage_error:.2f}%")

    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}

# Step 7.2: Evaluate both models
print("üéØ MODEL EVALUATION RESULTS:")
lin_metrics = evaluate_model(y_test, y_pred_lin, "Linear Regression")
tree_metrics = evaluate_model(y_test, y_pred_tree, "Decision Tree Regression")

# Step 7.3: Compare models
print("\nüèÜ MODEL COMPARISON:")
if lin_metrics['R2'] > tree_metrics['R2']:
    print("Linear Regression performs better!")
else:
    print("Decision Tree performs better!")

# Step 8.1: Create comprehensive visualization
plt.figure(figsize=(16, 6))

# Plot 1: Actual vs Predicted comparison
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred_lin, color='blue', label='Linear Regression', alpha=0.6, s=50)
plt.scatter(y_test, y_pred_tree, color='red', label='Decision Tree', alpha=0.6, s=50)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Perfect Prediction')
plt.xlabel("Actual Price (Lakhs)")
plt.ylabel("Predicted Price (Lakhs)")
plt.title("Model Prediction Comparison - Dhaka Housing")
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 2: Residual plot
plt.subplot(1, 2, 2)
residuals_lin = y_test - y_pred_lin
residuals_tree = y_test - y_pred_tree
plt.scatter(y_pred_lin, residuals_lin, color='blue', label='Linear Regression', alpha=0.6)
plt.scatter(y_pred_tree, residuals_tree, color='red', label='Decision Tree', alpha=0.6)
plt.axhline(y=0, color='black', linestyle='--')
plt.xlabel("Predicted Price (Lakhs)")
plt.ylabel("Residuals (Actual - Predicted)")
plt.title("Residual Plot")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Step 8.2: Feature importance for Decision Tree
if hasattr(tree_reg, 'feature_importances_'):
    print("\nüîç DECISION TREE FEATURE IMPORTANCE:")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': tree_reg.feature_importances_
    }).sort_values('importance', ascending=False)

    print(feature_importance)

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance, x='importance', y='feature')
    plt.title('Decision Tree - Feature Importance')
    plt.xlabel('Importance Score')
    plt.tight_layout()
    plt.show()

# Step 9.1: Example prediction with new data
print("\nüîÆ SAMPLE PREDICTION:")

# Take a sample from test set to demonstrate prediction
sample_idx = 0
sample_house = X_test.iloc[sample_idx:sample_idx+1]

print("Sample house features:")
print(sample_house)

# Make predictions
lin_pred = lin_reg.predict(sample_house)[0]
tree_pred = tree_reg.predict(sample_house)[0]
actual_price = y_test.iloc[sample_idx]

print(f"\nPrediction Results:")
print(f"Actual Price    : {actual_price:.2f} Lakhs")
print(f"Linear Reg Pred : {lin_pred:.2f} Lakhs")
print(f"Decision Tree Pred: {tree_pred:.2f} Lakhs")

# Calculate prediction errors
lin_error = abs(actual_price - lin_pred)
tree_error = abs(actual_price - tree_pred)

print(f"\nPrediction Errors:")
print(f"Linear Regression Error: {lin_error:.2f} Lakhs")
print(f"Decision Tree Error: {tree_error:.2f} Lakhs")

# Step 10.1: Final summary
print("\n" + "="*50)
print("üéâ PROJECT SUMMARY - DHAKA HOUSING PRICE PREDICTION")
print("="*50)

print(f"\nüìÅ Dataset: {df.shape[0]} houses, {df.shape[1]} features")
print(f"üéØ Target Variable: {y.name}")
print(f"üìä Training Samples: {X_train.shape[0]}")
print(f"üß™ Testing Samples: {X_test.shape[0]}")

print(f"\nüìà MODEL PERFORMANCE SUMMARY:")
print(f"Linear Regression R¬≤: {lin_metrics['R2']:.3f}")
print(f"Decision Tree R¬≤    : {tree_metrics['R2']:.3f}")

print(f"\nüí° INTERPRETATION:")
if lin_metrics['R2'] > 0.7:
    print("‚úì Excellent model performance!")
elif lin_metrics['R2'] > 0.5:
    print("‚úì Good model performance")
else:
    print("‚úì Model needs improvement - consider feature engineering")

print(f"\nüöÄ NEXT STEPS:")
print("1. Try more advanced models (Random Forest, XGBoost)")
print("2. Perform feature engineering")
print("3. Hyperparameter tuning")
print("4. Cross-validation")
print("5. Deploy the best model")